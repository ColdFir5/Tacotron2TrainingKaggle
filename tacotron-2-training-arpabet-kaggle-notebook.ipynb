{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n# **Tacotron 2 Training (Arpabet Version)(Kaggle Notebook)**\n---\n<a href=\"https://github.com/NVIDIA/tacotron2\"> Tacotron 2 </a> | **Created by ColdFir3#9543 (Michael)**\n\n**MAKE SURE 'GPU' HAS BEEN SELECTED AS THE ACCELERATOR IN THE NOTEBOOK SETTINGS**\n\n---\n# TRAINING INSTRUCTIONS\n\n* **Make sure to make your own version of this notebook for each new model**\n\n* Transcription file should look be in this format for each wav (WITH PUNCTUATION): ```wavs/1.wav|[Text here].```\n\n* Dataset WAVS format should be: **22050HZ and 16 bit PCM**\n\n* **ONLY IMPORT YOUR DATASET ONCE** *(should consist of a folder called 'wavs' with all audio clips and another file for your transcriptions)*\n\nExample: \n\n```\nKaggle Dataset/\n          ├──wavs/\n          │    ├──1.wav\n          │    ├──2.wav\n          │    ├──3.wav\n          │    └──etc\n          └──transcription.txt\n```\n* **MAKE SURE TO RUN ALL AND ENTER REQUIRED DATA**\n\n* ***(VERY IMPORTANT)*** Once your model has been trained **DO NOT FORGET TO SAVE VERSION AND GO TO 'ADVANCED' AND CHECK 'ALWAYS SAVE OUTPUT'** so you do NOT LOSE progress\n---\n# CONTINUING TO TRAIN?\n* If returning back to continue training your model **MAKE SURE TO RUN ALL AND ENTER REQUIRED DATA**\n\n* Import your Model as a \"dataset\" **SEPERATELY** from the training dataset\n\n* **DON'T MAKE ANY OF THE SETTINGS DIFFERENT FROM YOUR PREVIOUS TRAINING**\n\n* Recommended to name your \"input\" for your AI Model to be the same name as the model itself **(WILL SAVE LOTS OF TROUBLE AT STEP 5.5)**\n\n* ***(VERY IMPORTANT)*** Once your model has been trained **DO NOT FORGET TO SAVE VERSION USING 'QUICK SAVE' AS THE OPTION AND GO TO 'ADVANCED' AND CHECK 'ALWAYS SAVE OUTPUT'** so you do NOT LOSE progress","metadata":{}},{"cell_type":"markdown","source":"---\n\n# (Optional) Check GPU type","metadata":{}},{"cell_type":"code","source":"!nvidia-smi -L","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# 1 Run the below block of code to download and install Tacotron 2 and dependancies","metadata":{}},{"cell_type":"code","source":"# Install Tacotron (Arphabet Training Version)\nimport os\n!pip install tensorflow==1.15.2\n!pip install -q unidecode tensorboardX\nif not os.path.isdir(\"/kaggle/working/tacotron2/\"):\n    !git clone -q https://github.com/NVIDIA/tacotron2.git\n    %cd tacotron2\n    !git submodule init\n    !git submodule update\n%cd tacotron2\n# NVIDIA's requirements\n## !pip3 install torch==1.9.1+cu102 torchvision==0.10.1+cu102 torchaudio===0.9.1 -f https://download.pytorch.org/whl/torch_stable.html\n!pip install inflect\n!apt-get install pv\n!apt-get install jq\n!pip install gdown\n!pip install matplotlib numpy inflect librosa scipy Unidecode pillow\n## %matplotlib inline\n# Download NVIDIA's LJSpeech model\ntt2_pretrained = \"https://drive.google.com/uc?id=1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA\"\nif not os.path.isfile(\"/kaggle/working/tacotron2/pretrained_model\"):\n    !gdown https://drive.google.com/uc?id=1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA -O pretrained_model\n#     gdown.download(tt2_pretrained, \"/content/tacotron2/pretrained_model\", quiet=False)\nif not os.path.isfile(\"/kaggle/working/tacotron2/text/merged.dict.txt\"):\n    !curl https://cdn.discordapp.com/attachments/820353681567907901/865742324084244480/tacotron2-cmudict-patch.zip -o /kaggle/working/tacotron2-cmudict-patch.zip\n    !unzip -o /kaggle/working/tacotron2-cmudict-patch.zip -d /kaggle/working/tacotron2/text/\n    !mv /kaggle/working/tacotron2/text/merged.dict.txt /kaggle/working/tacotron2/","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# 2 Apply more Arpabet patches","metadata":{}},{"cell_type":"code","source":"patches = [\n  (\"/kaggle/working/tacotron2/train.patch\", \"\"\"17a18\n> from tqdm import tqdm\n18a20,26\n> from plotting_utils import plot_alignment_to_numpy\n> from IPython import display\n> from PIL import Image\n> \n> def plot_something_to_ipython(alignment):\n>   numpoop = plot_alignment_to_numpy(alignment)\n>   display.display(Image.fromarray(numpoop))\n28,29c36,37\n<     assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n<     print(\"Initializing Distributed\")\n---\n>     assert torch.cuda.is_available(), \"distributed mode requires CUDA\"\n>     print(\"initializing distributed\")\n39c47\n<     print(\"Done initializing distributed\")\n---\n>     print(\"done initializing distributed\")\n86c94\n<     print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n---\n>     print(\"warm starting model from checkpoint '{}'\".format(checkpoint_path))\n101c109\n<     print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n---\n>     print(\"loading checkpoint '{}'\".format(checkpoint_path))\n107c115\n<     print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n---\n>     print(\"loaded checkpoint '{}' from iteration {}\" .format(\n113c121\n<     print(\"Saving model and optimizer state at iteration {} to {}\".format(\n---\n>     print(\"saving model and optimizer state at iteration {} to {}\".format(\n145c153\n<         print(\"Validation loss {}: {:9f}  \".format(iteration, val_loss))\n---\n>         print(\"validation loss {}: {:9f}  \".format(iteration, val_loss))\n146a155,161\n>         \n>         # plot_something_to_ipython\n>         # https://github.com/NVIDIA/tacotron2/blob/master/logger.py\n>         _, _, _, alignments = y_pred\n>         from random import randint\n>         idx = randint(0, alignments.size(0)-1)\n>         plot_something_to_ipython(alignments[idx].data.cpu().numpy().T)\n169a185\n>     print(\"starting with {} learning rate\".format(learning_rate))\n207,208c223,225\n<         print(\"Epoch: {}\".format(epoch))\n<         for i, batch in enumerate(train_loader):\n---\n>         print(\"starting epoch {} at iteration {}\".format(epoch, iteration))\n>         epochstart = time.perf_counter()\n>         for i, batch in tqdm(enumerate(train_loader)):\n240,241c257,258\n<                 print(\"Train loss {} {:.6f} Grad Norm {:.6f} {:.2f}s/it\".format(\n<                     iteration, reduced_loss, grad_norm, duration))\n---\n>                 #print(\"Train loss {} {:.6f} Grad Norm {:.6f} {:.2f}s/it\".format(\n>                 #    iteration, reduced_loss, grad_norm, duration))\n243a261\n>             from random import random\n245,246c263,266\n<             if not is_overflow and (iteration % hparams.iters_per_checkpoint == 0):\n<                 validate(model, criterion, valset, iteration,\n---\n>             iteration += 1\n>         print(\"\\nepoch {} took {} seconds\".format(epoch, time.perf_counter() - epochstart))\n>         #what could possibly go wrong\n>         validate(model, criterion, valset, iteration,\n248a269\n>         if not is_overflow and (random() > 0.66): #(iteration % hparams.iters_per_checkpoint == 0):\n251,256c272,280\n<                         output_directory, \"checkpoint_{}\".format(iteration))\n<                     save_checkpoint(model, optimizer, learning_rate, iteration,\n<                                     checkpoint_path)\n< \n<             iteration += 1\n< \n---\n>                         output_directory, hparams.model_name)\n>                     try:\n>                         save_checkpoint(model, optimizer, learning_rate, iteration,\n>                                         checkpoint_path)\n>                     except KeyboardInterrupt:\n>                         print(\"you probably shouldnt ^C while im saving\")\n>                         save_checkpoint(model, optimizer, learning_rate, iteration,\n>                                         checkpoint_path)\n>                         print(\"ok it should be fine now\")\n\n\"\"\"),\n  (\"/kaggle/working/tacotron2/plotting_utils.patch\", \"\"\"5c5\n< \n---\n> import io\n9,10c9,13\n<     data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n<     data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n---\n>     # no obvious way to make it rgba... https://github.com/matplotlib/matplotlib/issues/5336#issuecomment-388736185\n>     buf = io.BytesIO()\n>     fig.savefig(buf, format=\"rgba\")\n>     data = np.fromstring(buf.getvalue(), dtype=np.uint8, sep='')\n>     data = data.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n15c18\n<     fig, ax = plt.subplots(figsize=(6, 4))\n---\n>     fig, ax = plt.subplots(figsize=(9, 6))\n17c20\n<                    interpolation='none')\n---\n>                    interpolation='none', cmap='inferno')\n35c38\n<                    interpolation='none')\n---\n>                    interpolation='none', cmap='inferno')\n\n\"\"\")\n]\nfor i, v in enumerate(patches):\n  to = v[0]\n  co = v[1]\n  with open(to, \"w\") as file:\n    file.write(co)\n\nfrom glob import glob \nfor x in glob(\"*.patch\"):\n  base = x[:-6]\n  patch = x\n  py = base+\".py\"\n  !patch {py} {patch}\n  import time\nimport argparse\nimport math\nfrom distutils.dir_util import copy_tree\nfrom numpy import finfo\n\nimport torch\nfrom distributed import apply_gradient_allreduce\nimport torch.distributed as dist\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data import DataLoader\n\nfrom model import Tacotron2\nfrom data_utils import TextMelLoader, TextMelCollate\nfrom loss_function import Tacotron2Loss\nfrom logger import Tacotron2Logger\nfrom hparams import create_hparams\n \nimport random\nimport numpy as np\n\nimport layers\nfrom utils import load_wav_to_torch, load_filepaths_and_text\nfrom text import text_to_sequence\nfrom math import e\nfrom tqdm.notebook import tqdm # Modern Notebook TQDM\nfrom distutils.dir_util import copy_tree\nimport matplotlib.pylab as plt\n\ndef create_mels():\n    print(\"Generating Mels\")\n    stft = layers.TacotronSTFT(\n                hparams.filter_length, hparams.hop_length, hparams.win_length,\n                hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n                hparams.mel_fmax)\n    def save_mel(filename):\n        audio, sampling_rate = load_wav_to_torch(filename)\n        if sampling_rate != stft.sampling_rate:\n            raise ValueError(\"{} {} SR doesn't match target {} SR\".format(filename, \n                sampling_rate, stft.sampling_rate))\n        audio_norm = audio / hparams.max_wav_value\n        audio_norm = audio_norm.unsqueeze(0)\n        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n        melspec = stft.mel_spectrogram(audio_norm)\n        melspec = torch.squeeze(melspec, 0).cpu().numpy()\n        np.save(filename.replace('.wav', ''), melspec)\n\n    import glob\n    wavs = glob.glob('/wavs/*.wav')\n    for i in tqdm(wavs):\n        save_mel(i)\n\n\ndef reduce_tensor(tensor, n_gpus):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n    rt /= n_gpus\n    return rt\n\n\ndef init_distributed(hparams, n_gpus, rank, group_name):\n    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n    print(\"Initializing Distributed\")\n\n    # Set cuda device so everything is done on the right GPU.\n    torch.cuda.set_device(rank % torch.cuda.device_count())\n\n    # Initialize distributed communication\n    dist.init_process_group(\n        backend=hparams.dist_backend, init_method=hparams.dist_url,\n        world_size=n_gpus, rank=rank, group_name=group_name)\n\n    print(\"Done initializing distributed\")\n\n\ndef prepare_dataloaders(hparams):\n    # Get data, data loaders and collate function ready\n    trainset = TextMelLoader(hparams.training_files, hparams)\n    valset = TextMelLoader(hparams.validation_files, hparams)\n    collate_fn = TextMelCollate(hparams.n_frames_per_step)\n\n    if hparams.distributed_run:\n        train_sampler = DistributedSampler(trainset)\n        shuffle = False\n    else:\n        train_sampler = None\n        shuffle = True\n\n    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,\n                              sampler=train_sampler,\n                              batch_size=hparams.batch_size, pin_memory=False,\n                              drop_last=True, collate_fn=collate_fn)\n    return train_loader, valset, collate_fn\n\n\ndef prepare_directories_and_logger(output_directory, log_directory, rank):\n    if rank == 0:\n        if not os.path.isdir(output_directory):\n            os.makedirs(output_directory)\n            os.chmod(output_directory, 0o775)\n        logger = Tacotron2Logger(os.path.join(output_directory, log_directory))\n    else:\n        logger = None\n    return logger\n\n\ndef load_model(hparams):\n    model = Tacotron2(hparams).cuda()\n    if hparams.fp16_run:\n        model.decoder.attention_layer.score_mask_value = finfo('float16').min\n\n    if hparams.distributed_run:\n        model = apply_gradient_allreduce(model)\n\n    return model\n\n\ndef warm_start_model(checkpoint_path, model, ignore_layers):\n    assert os.path.isfile(checkpoint_path)\n    print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n    model_dict = checkpoint_dict['state_dict']\n    if len(ignore_layers) > 0:\n        model_dict = {k: v for k, v in model_dict.items()\n                      if k not in ignore_layers}\n        dummy_dict = model.state_dict()\n        dummy_dict.update(model_dict)\n        model_dict = dummy_dict\n    model.load_state_dict(model_dict)\n    return model\n\n\ndef load_checkpoint(checkpoint_path, model, optimizer):\n    assert os.path.isfile(checkpoint_path)\n    print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n    model.load_state_dict(checkpoint_dict['state_dict'])\n    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n    learning_rate = checkpoint_dict['learning_rate']\n    iteration = checkpoint_dict['iteration']\n    print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n        checkpoint_path, iteration))\n    return model, optimizer, learning_rate, iteration\n\n\ndef save_checkpoint(model, optimizer, learning_rate, iteration, filepath):\n    print(\"Saving model and optimizer state at iteration {} to {}\".format(\n        iteration, filepath))\n    try:\n        torch.save({'iteration': iteration,\n                'state_dict': model.state_dict(),\n                'optimizer': optimizer.state_dict(),\n                'learning_rate': learning_rate}, filepath)\n    except KeyboardInterrupt:\n        print(\"interrupt received while saving, waiting for save to complete.\")\n        torch.save({'iteration': iteration,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(),'learning_rate': learning_rate}, filepath)\n    print(\"Model Saved\")\n\ndef plot_alignment(alignment, info=None):\n    %matplotlib inline\n    fig, ax = plt.subplots(figsize=(int(alignment_graph_width/100), int(alignment_graph_height/100)))\n    im = ax.imshow(alignment, cmap='inferno', aspect='auto', origin='lower',\n                   interpolation='none')\n    ax.autoscale(enable=True, axis=\"y\", tight=True)\n    fig.colorbar(im, ax=ax)\n    xlabel = 'Decoder timestep'\n    if info is not None:\n        xlabel += '\\n\\n' + info\n    plt.xlabel(xlabel)\n    plt.ylabel('Encoder timestep')\n    plt.tight_layout()\n    fig.canvas.draw()\n    plt.show()\n\ndef validate(model, criterion, valset, iteration, batch_size, n_gpus,\n             collate_fn, logger, distributed_run, rank, epoch, start_eposh, learning_rate):\n    \"\"\"Handles all the validation scoring and printing\"\"\"\n    model.eval()\n    with torch.no_grad():\n        val_sampler = DistributedSampler(valset) if distributed_run else None\n        val_loader = DataLoader(valset, sampler=val_sampler, num_workers=1,\n                                shuffle=False, batch_size=batch_size,\n                                pin_memory=False, collate_fn=collate_fn)\n\n        val_loss = 0.0\n        for i, batch in enumerate(val_loader):\n            x, y = model.parse_batch(batch)\n            y_pred = model(x)\n            loss = criterion(y_pred, y)\n            if distributed_run:\n                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()\n            else:\n                reduced_val_loss = loss.item()\n            val_loss += reduced_val_loss\n        val_loss = val_loss / (i + 1)\n\n    model.train()\n    if rank == 0:\n        print(\"Epoch: {} Validation loss {}: {:9f}  Time: {:.1f}m LR: {:.6f}\".format(epoch, iteration, val_loss,(time.perf_counter()-start_eposh)/60, learning_rate))\n        logger.log_validation(val_loss, model, y, y_pred, iteration)\n        if hparams.show_alignments:\n            %matplotlib inline\n            _, mel_outputs, gate_outputs, alignments = y_pred\n            idx = random.randint(0, alignments.size(0) - 1)\n            plot_alignment(alignments[idx].data.cpu().numpy().T)\n\ndef train(output_directory, log_directory, checkpoint_path, warm_start, n_gpus,\n          rank, group_name, hparams, log_directory2):\n    \"\"\"Training and validation logging results to tensorboard and stdout\n\n    Params\n    ------\n    output_directory (string): directory to save checkpoints\n    log_directory (string) directory to save tensorboard logs\n    checkpoint_path(string): checkpoint path\n    n_gpus (int): number of gpus\n    rank (int): rank of current gpu\n    hparams (object): comma separated list of \"name=value\" pairs.\n    \"\"\"\n    if hparams.distributed_run:\n        init_distributed(hparams, n_gpus, rank, group_name)\n\n    torch.manual_seed(hparams.seed)\n    torch.cuda.manual_seed(hparams.seed)\n\n    model = load_model(hparams)\n    learning_rate = hparams.learning_rate\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n                                 weight_decay=hparams.weight_decay)\n\n    if hparams.fp16_run:\n        from apex import amp\n        model, optimizer = amp.initialize(\n            model, optimizer, opt_level='O2')\n\n    if hparams.distributed_run:\n        model = apply_gradient_allreduce(model)\n\n    criterion = Tacotron2Loss()\n\n    logger = prepare_directories_and_logger(\n        output_directory, log_directory, rank)\n\n    train_loader, valset, collate_fn = prepare_dataloaders(hparams)\n\n    # Load checkpoint if one exists\n    iteration = 0\n    epoch_offset = 0\n    if checkpoint_path is not None and os.path.isfile(checkpoint_path):\n        if warm_start:\n            model = warm_start_model(\n                checkpoint_path, model, hparams.ignore_layers)\n        else:\n            model, optimizer, _learning_rate, iteration = load_checkpoint(\n                checkpoint_path, model, optimizer)\n            if hparams.use_saved_learning_rate:\n                learning_rate = _learning_rate\n            iteration += 1  # next iteration is iteration + 1\n            epoch_offset = max(0, int(iteration / len(train_loader)))\n    else:\n    # download LJSpeech pretrained model if no checkpoint already exists\n      os.path.isfile(\"pretrained_model\")\n#       %cd /kaggle/working\n#       !/output/kaggle/working/tacotron2/megadown.sh https://mega.nz/#!WXY3RILA!KyoGHtfB_sdhmLFoykG2lKWhh0GFdwMkk7OwAjpQHRo --o pretrained_model\n      !gdown https://drive.google.com/uc?id=1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA -O pretrained_model\n#       %cd tacotron2\n      model = warm_start_model(\"pretrained_model\", model, hparams.ignore_layers)\n      \n    \n    start_eposh = time.perf_counter()\n    learning_rate = 0.0\n    model.train()\n    is_overflow = False\n    # ================ MAIN TRAINNIG LOOP! ===================\n    for epoch in tqdm(range(epoch_offset, hparams.epochs)):\n        print(\"\\nStarting Epoch: {} Iteration: {}\".format(epoch, iteration))\n        start_eposh = time.perf_counter() # eposh is russian, not a typo\n        for i, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n            start = time.perf_counter()\n            if iteration < hparams.decay_start: learning_rate = hparams.A_\n            else: iteration_adjusted = iteration - hparams.decay_start; learning_rate = (hparams.A_*(e**(-iteration_adjusted/hparams.B_))) + hparams.C_\n            learning_rate = max(hparams.min_learning_rate, learning_rate) # output the largest number\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = learning_rate\n\n            model.zero_grad()\n            x, y = model.parse_batch(batch)\n            y_pred = model(x)\n\n            loss = criterion(y_pred, y)\n            if hparams.distributed_run:\n                reduced_loss = reduce_tensor(loss.data, n_gpus).item()\n            else:\n                reduced_loss = loss.item()\n            if hparams.fp16_run:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            if hparams.fp16_run:\n                grad_norm = torch.nn.utils.clip_grad_norm_(\n                    amp.master_params(optimizer), hparams.grad_clip_thresh)\n                is_overflow = math.isnan(grad_norm)\n            else:\n                grad_norm = torch.nn.utils.clip_grad_norm_(\n                    model.parameters(), hparams.grad_clip_thresh)\n\n            optimizer.step()\n\n            if not is_overflow and rank == 0:\n                duration = time.perf_counter() - start\n                logger.log_training(\n                    reduced_loss, grad_norm, learning_rate, duration, iteration)\n                #print(\"Batch {} loss {:.6f} Grad Norm {:.6f} Time {:.6f}\".format(iteration, reduced_loss, grad_norm, duration), end='\\r', flush=True)\n\n            iteration += 1\n        validate(model, criterion, valset, iteration,\n                 hparams.batch_size, n_gpus, collate_fn, logger,\n                 hparams.distributed_run, rank, epoch, start_eposh, learning_rate)\n        save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path)\n        if log_directory2 != None:\n            copy_tree(log_directory, log_directory2)\ndef check_dataset(hparams):\n    from utils import load_wav_to_torch, load_filepaths_and_text\n    import os\n    import numpy as np\n    def check_arr(filelist_arr):\n        for i, file in enumerate(filelist_arr):\n            if len(file) > 2:\n                print(\"|\".join(file), \"\\nhas multiple '|', this may not be an error.\")\n            if hparams.load_mel_from_disk and '.wav' in file[0]:\n                print(\"[WARNING]\", file[0], \" in filelist while expecting .npy .\")\n            else:\n                if not hparams.load_mel_from_disk and '.npy' in file[0]:\n                    print(\"[WARNING]\", file[0], \" in filelist while expecting .wav .\")\n            if (not os.path.exists(file[0])):\n                print(\"|\".join(file), \"\\n[WARNING] does not exist.\")\n            if len(file[1]) < 3:\n                print(\"|\".join(file), \"\\n[info] has no/very little text.\")\n            if not ((file[1].strip())[-1] in r\"!?,.;:\"):\n                print(\"|\".join(file), \"\\n[info] has no ending punctuation.\")\n            mel_length = 1\n            if hparams.load_mel_from_disk and '.npy' in file[0]:\n                melspec = torch.from_numpy(np.load(file[0], allow_pickle=True))\n                mel_length = melspec.shape[1]\n            if mel_length == 0:\n                print(\"|\".join(file), \"\\n[WARNING] has 0 duration.\")\n    print(\"Checking Training Files\")\n    audiopaths_and_text = load_filepaths_and_text(hparams.training_files) # get split lines from training_files text file.\n    check_arr(audiopaths_and_text)\n    print(\"Checking Validation Files\")\n    audiopaths_and_text = load_filepaths_and_text(hparams.validation_files) # get split lines from validation_files text file.\n    check_arr(audiopaths_and_text)\n    print(\"Finished Checking\")\n\nwarm_start=False\nn_gpus=1\nrank=0\ngroup_name=None\n\n# ---- DEFAULT PARAMETERS DEFINED HERE ----\nhparams = create_hparams()\nmodel_filename = 'current_model'\nhparams.training_files = \"filelists/clipper_train_filelist.txt\"\nhparams.validation_files = \"filelists/clipper_val_filelist.txt\"\nhparams.p_attention_dropout=0.1\nhparams.p_decoder_dropout=0.1\nhparams.decay_start = 15000\nhparams.A_ = 5e-4\nhparams.B_ = 8000\nhparams.C_ = 0\nhparams.min_learning_rate = 1e-5\ngenerate_mels = True\nhparams.show_alignments = True\nalignment_graph_height = 600\nalignment_graph_width = 1000\nhparams.batch_size = 32\nhparams.load_mel_from_disk = True\nhparams.ignore_layers = []\nhparams.epochs = 10000\ntorch.backends.cudnn.enabled = hparams.cudnn_enabled\ntorch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n\ndata_path = 'wavs'\n!mkdir {data_path}","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# 3 Create file lists from train and validation sets\nEnter the name of your dataset and the name of your training transcription","metadata":{}},{"cell_type":"code","source":"# Variables\ndataset_name = \"none\"\ntraining_list_name = \"none\"\n\n# Inputs\nwhile dataset_name == \"none\":\n    dataset_name = input(\"What is the name of your dataset?: \")\n\nwhile training_list_name == \"none\":\n    training_list_name = input(\"What is the name of your training transcription file (add file extention: eg .txt)?: \")\n\n# Training list\nwith open(f\"../../input/{dataset_name}/{training_list_name}\") as f:\n    with open(f\"filelists/{training_list_name}\", \"w\") as f1:\n        for line in f:\n            f1.write(f'/'+line)\n\n# User Completion\nprint(\"Done, text file has been copied in the right location.\")","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# 4 Setting Model Parameters\nEnter your model name and your EPOCH limit to train toward","metadata":{}},{"cell_type":"code","source":"model_filename = \"none\"\nwhile model_filename == \"none\":\n    model_filename = input(\"What is your desired model name?: \")\n\nTraining_file = f\"filelists/{training_list_name}\"\n    \nhparams.training_files = Training_file\nhparams.validation_files = Training_file\n\n# hparams to Tune\nhparams.p_attention_dropout=0.1\nhparams.p_decoder_dropout=0.1\n\n# Learning Rate\nhparams.decay_start = 15000         # wait till decay_start to start decaying learning rate\nhparams.A_ = 5e-4                   # Start/Max Learning Rate\nhparams.B_ = 8000                   # Decay Rate\nhparams.C_ = 0                      # Shift learning rate equation by this value\nhparams.min_learning_rate = 1e-5    # Min Learning Rate\n\n# Quality of Life\ngenerate_mels = True\nhparams.show_alignments = True\nalignment_graph_height = 600\nalignment_graph_width = 1000\n\n#Your batch size, lower if you don't have enough ram.\nhparams.batch_size = 14 #18\nhparams.batch_size = -1\nwhile hparams.batch_size <= 0:\n    try:\n        hparams.batch_size = int(input(\"Enter the batch size you wish to use (Recommended: 14): \"))\n    except ValueError:\n        print(\"Please try again\")\n\nhparams.load_mel_from_disk = True\nhparams.ignore_layers = [] # Layers to reset (None by default, other than foreign languages this param can be ignored)\nuse_cmudict = True # Train the model using the dictionary (BOOLEAN)\n\nhparams.epochs = -1\nwhile hparams.epochs <= -1:\n    try:\n        hparams.epochs = int(input(\"Your total epochs to train to (recommended: 200): \"))\n    except ValueError:\n        print(\"Please try again\")\n\ntorch.backends.cudnn.enabled = hparams.cudnn_enabled\ntorch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n\noutput_directory = '../outdir'\nlog_directory = '/logs' # Location to save Log files locally\nlog_directory2 = '../outdir/logs' # Location to copy log files (done at the end of each epoch to cut down on I/O)\ncheckpoint_path = output_directory+(r'/')+model_filename\n\n# User Completion\nprint(\"Done, Settings applied\")","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# 5 Move files from Dataset to working directory","metadata":{}},{"cell_type":"code","source":"os.system(f'cp -a ../../input/{dataset_name}/wavs /')\nprint(\"Files have successfully been copied over\")","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# 6 Convert WAV files into Mel-Spectograms and Checks missing files","metadata":{}},{"cell_type":"code","source":"print(\"Generating mels\")\nif generate_mels:\n    create_mels()\n\nprint(\"Checking for missing files\")\n# ---- Replace .wav with .npy in filelists ----\n!sed -i -- 's,.wav|,.npy|,g' {hparams.training_files}; sed -i -- 's,.wav|,.npy|,g' {hparams.validation_files}\n\ncheck_dataset(hparams)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# 7 Check if cmudict patch is working","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/tacotron2/\nimport text\nfor i in range(10):\n  print(text.sequence_to_text(text.text_to_sequence(\"We are checking to see if this works or not.\", [\"cmudict_cleaners\", \"english_cleaners\"])))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# **(CONTINUING TO TRAIN?)** 7.5 Transfer Model into the working directory","metadata":{}},{"cell_type":"code","source":"CONTINUEQ = input(\"Are you continuing to train your model? [Y/N]: \").upper()\nwhile CONTINUEQ not in (\"Y\",\"N\"):\n    CONTINUEQ = input(\"Please only enter Y or N\\nAre you continuing to train your model? [Y/N]: \")\n\nif CONTINUEQ == \"Y\":\n    os.system('mkdir ../outdir')\n    print(\"NOT THE MODEL FILENAME ITSELF, ONLY THE NAME YOU HAD GIVEN IT WHEN IMPORTING AS DATA INTO KAGGLE\")\n    MODELDATA_FILENAME = input(\"What is the name of the name you had assigned your TACOTRON2 MODEL within your KAGGLE ACCOUNT?: \")\n    os.system(f'cp -r ../../input/{MODELDATA_FILENAME}/{model_filename} ../outdir/')\nelse:\n    print(\"Ok\")","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# 8 Train the model","metadata":{}},{"cell_type":"code","source":"print(\"FP16 Run:\", hparams.fp16_run)\nprint(\"Dynamic Loss Scaling:\", hparams.dynamic_loss_scaling)\nprint(\"Distributed Run:\", hparams.distributed_run)\nprint(\"cuDNN Enabled:\", hparams.cudnn_enabled)\nprint(\"cuDNN Benchmark:\", hparams.cudnn_benchmark)\ntrain(output_directory, log_directory, checkpoint_path, warm_start, n_gpus, rank, group_name, hparams, log_directory2)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# 9 Download Model\n\n**DOUBLE CLICK THIS BOX AND CHANGE 'EDIT' to the name of your model, then click outside the box and click 'Download File'**\n\n<a href=\"../outdir/EDIT\"> Download File </a>","metadata":{}},{"cell_type":"markdown","source":"---\n\n# **Good training will look like this:**","metadata":{}},{"cell_type":"markdown","source":"![img.png](https://media.discordapp.net/attachments/835971020569051216/851469553355587614/download_2.png)","metadata":{}}]}